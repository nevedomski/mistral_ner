# Default configuration for Mistral NER fine-tuning

model:
  model_name: "mistralai/Mistral-7B-v0.3"
  num_labels: 9
  # Quantization settings - Choose one:
  # - 8-bit: Better performance, ~10GB VRAM usage
  # - 4-bit: More memory efficient, ~6GB VRAM usage
  # - Both false: Full precision, ~24GB VRAM usage
  load_in_8bit: false   # Set to true for 8-bit quantization
  load_in_4bit: true    # Set to true for 4-bit quantization (QLoRA) - DEFAULT
  device_map: "auto"
  trust_remote_code: true
  use_cache: false
  
  # LoRA configuration - Enhanced for better capacity
  lora_r: 32  # Increased from 16 for better capacity
  lora_alpha: 64  # Increased proportionally (2x rank)
  lora_dropout: 0.1
  lora_bias: "none"
  target_modules:
    - "q_proj"      # Query projection
    - "k_proj"      # Key projection  
    - "v_proj"      # Value projection
    - "o_proj"      # Output projection
    - "gate_proj"   # MLP gate projection
    - "up_proj"     # MLP up projection
    - "down_proj"   # MLP down projection
  task_type: "TOKEN_CLS"

data:
  dataset_name: "conll2003"
  max_length: 256
  label_all_tokens: false
  return_entity_level_metrics: true
  
  # Label configuration
  label_names:
    - "O"
    - "B-PER"
    - "I-PER"
    - "B-ORG"
    - "I-ORG"
    - "B-LOC"
    - "I-LOC"
    - "B-MISC"
    - "I-MISC"

training:
  output_dir: "./mistral-ner-finetuned"
  final_output_dir: "./mistral-ner-finetuned-final"
  num_train_epochs: 5
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  optim: "paged_adamw_32bit"
  learning_rate: 0.0002
  weight_decay: 0.001
  warmup_ratio: 0.03
  max_grad_norm: 1.0
  
  # Loss function configuration
  loss_type: "focal"  # focal, cross_entropy, label_smoothing, class_balanced
  focal_alpha: null   # Auto-computed from class frequencies if null
  focal_gamma: 2.0    # Focusing parameter for focal loss
  label_smoothing: 0.1  # Only used if loss_type='label_smoothing'
  class_balanced_beta: 0.9999  # Only used if loss_type='class_balanced'
  
  # Learning rate scheduling
  lr_scheduler_type: "cosine"  # linear, cosine, cosine_with_restarts, polynomial
  lr_scheduler_kwargs: {}  # Additional scheduler parameters
  
  # Training strategy
  eval_strategy: "epoch"
  save_strategy: "epoch"
  logging_steps: 10
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_f1"
  greater_is_better: true
  report_to:
    - "wandb"
  
  # Mixed precision (auto-detected)
  fp16: false
  bf16: false
  tf32: true
  
  # Early stopping
  early_stopping_patience: 3
  early_stopping_threshold: 0.01
  
  # Memory management
  clear_cache_steps: 50
  
  # Resume from checkpoint
  resume_from_checkpoint: null
  
  # Seed
  seed: 42
  data_seed: 42
  
  # Hub
  push_to_hub: false
  hub_model_id: null
  hub_strategy: "every_save"
  
  # Model saving
  merge_adapters_on_save: true  # Merge LoRA adapters for deployment-ready model

logging:
  log_level: "info"
  log_dir: "./logs"
  disable_tqdm: false
  
  # WandB configuration
  use_wandb: true
  wandb_project: "mistral-ner"
  wandb_entity: null  # Set via WANDB_ENTITY env var or specify here
  wandb_name: null    # Auto-generated if not specified
  wandb_tags: []      # Add tags like ["experiment", "baseline"]
  wandb_notes: null   # Add experiment notes
  wandb_mode: "online"  # online, offline, disabled
  wandb_dir: "./wandb_logs"  # Directory for offline runs
  wandb_resume: "allow"  # allow, must, never, auto
  wandb_run_id: null  # For resuming specific runs
  wandb_api_key: null  # Set via WANDB_API_KEY env var or specify here